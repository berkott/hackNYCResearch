{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing around with the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import umap\n",
    "from umap.umap_ import UMAP\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "from datasets import load_dataset\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "SUBSAMPLE_N = 10\n",
    "SAVEDIR = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for model:  EleutherAI_gpt-j-6B\n",
      "found cached embeddings for model:  reciprocate_ppo_hh_gpt-j\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/umap/spectral.py:260: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "def make_sphere_data(model_strings):\n",
    "    # if os.path.exists('./data/figure_4_sphere_data.pkl'):\n",
    "    #     print('Found cached sphere data')\n",
    "    #     return\n",
    "\n",
    "    generations_by_model_string = {}\n",
    "    pre_umap = []\n",
    "    for model_string in model_strings:\n",
    "        save_string = model_string.replace('/', '_')\n",
    "\n",
    "        with open('./data/' + save_string + '_fig4_embeddings.pkl', 'rb') as f:\n",
    "            all_embs = pickle.load(f)\n",
    "            for prompt in all_embs:\n",
    "                for sequence in prompt:\n",
    "                    sequence = np.squeeze(sequence)\n",
    "                    sequence = np.stack(sequence).astype('float64')\n",
    "                    sequence = sequence / np.linalg.norm(sequence, axis=1, keepdims=True)\n",
    "                    sequence = np.cumsum(sequence, axis=0) / np.arange(1, sequence.shape[0]+1).reshape(-1, 1)\n",
    "                    pre_umap.append(np.stack(sequence))\n",
    "            generations_by_model_string[model_string] = all_embs\n",
    "\n",
    "    pre_umap = np.concatenate(pre_umap, axis=0)\n",
    "    # sphere_mapper = umap.UMAP(output_metric='haversine').fit(pre_umap)\n",
    "    sphere_mapper = UMAP(output_metric='haversine').fit(pre_umap)\n",
    "    x = np.sin(sphere_mapper.embedding_[:, 0]) * np.cos(sphere_mapper.embedding_[:, 1])\n",
    "    y = np.sin(sphere_mapper.embedding_[:, 0]) * np.sin(sphere_mapper.embedding_[:, 1])\n",
    "    z = np.cos(sphere_mapper.embedding_[:, 0])\n",
    "\n",
    "    pointer = 0\n",
    "    sphere_data_by_model_string = {model_string: [] for model_string in model_strings}\n",
    "    for model_string in model_strings:\n",
    "        for prompt in generations_by_model_string[model_string]:\n",
    "            cur_prompt_continuations = []\n",
    "            for sequence in prompt:\n",
    "                cur_generation_points = []\n",
    "                for _ in sequence:\n",
    "                    cur_generation_points.append([x[pointer], y[pointer], z[pointer]])\n",
    "                    pointer += 1\n",
    "                cur_prompt_continuations.append(cur_generation_points)\n",
    "            sphere_data_by_model_string[model_string].append(cur_prompt_continuations)\n",
    "\n",
    "    return sphere_data_by_model_string\n",
    "    # with open('./data/figure_4_sphere_data.pkl', 'wb') as f:\n",
    "    #     pickle.dump(sphere_data_by_model_string, f)\n",
    "    # with open('./data/figure_4_sphere_data2.pkl', 'wb') as f:\n",
    "    #     pickle.dump(sphere_data_by_model_string, f)\n",
    "\n",
    "\n",
    "def get_data(model_string):\n",
    "    save_string = model_string.replace('/', '_')\n",
    "    if os.path.exists('./data/' + save_string + '_fig4_embeddings.pkl'):\n",
    "        print('found cached embeddings for model: ', save_string)\n",
    "        with open('./data/' + save_string + '_fig4_embeddings.pkl', 'rb') as f:\n",
    "            d = pickle.load(f)\n",
    "            # return [e for e in d[:10]]\n",
    "            return [e for e in d[:SUBSAMPLE_N]] # TODO: See if this was right change\n",
    "\n",
    "    print('loading model')\n",
    "    model = GPTJForCausalLM.from_pretrained(model_string,\n",
    "                                            output_hidden_states=True,\n",
    "                                            torch_dtype=torch.float16,\n",
    "                                            low_cpu_mem_usage=True)\n",
    "    model.cuda()\n",
    "    print('loading tokenizer')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_string)\n",
    "    print('loading dataset')\n",
    "    ds = load_dataset('allenai/prosocial-dialog', split='test')\n",
    "    print('formatting dataset')\n",
    "\n",
    "    chosen_ds = [e['context'] for e in ds][:SUBSAMPLE_N]\n",
    "    # chosen_ds = [e['context'] for e in ds]\n",
    "    all_embeddings = []\n",
    "    all_generations = []\n",
    "    for e in tqdm(chosen_ds):\n",
    "        input = tokenizer(e, return_tensors='pt', truncation=True, max_length=256)\n",
    "        input = {k: v.cuda() for k, v in input.items()}\n",
    "\n",
    "\n",
    "        n_new_tokens = 20\n",
    "        cur_embeddings = []\n",
    "        cur_generations = []\n",
    "        for i in range(5):\n",
    "            generated_tokens = model.generate(input['input_ids'],\n",
    "                                              attention_mask=input['attention_mask'],\n",
    "                                              max_length=input['input_ids'].shape[1] + n_new_tokens,\n",
    "                                              min_new_tokens=10,\n",
    "                                              num_beams=5,\n",
    "                                              repetition_penalty=2.0,\n",
    "                                              do_sample=True,\n",
    "                                              num_return_sequences=1)\n",
    "\n",
    "\n",
    "            generated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "            cur_generations.append(generated_text)\n",
    "\n",
    "            # Concatenate the new tokens to the original input_ids\n",
    "            extended_input_ids = torch.cat((input['input_ids'], generated_tokens[:, -n_new_tokens:]), dim=1)\n",
    "            extended_attention_mask = torch.cat(\n",
    "                (input[\"attention_mask\"], torch.ones_like(generated_tokens[:, -n_new_tokens:]).cuda()), dim=1)\n",
    "\n",
    "            extended_input = {\"input_ids\": extended_input_ids, \"attention_mask\": extended_attention_mask}\n",
    "\n",
    "            y = model(**extended_input)\n",
    "            embedding = y.hidden_states[0].detach().cpu().numpy()[0]\n",
    "            cur_embeddings.append(embedding)\n",
    "            cur_generations.append(generated_text)\n",
    "\n",
    "        cur_embeddings = np.stack(cur_embeddings)\n",
    "        all_embeddings.append(cur_embeddings)\n",
    "        all_generations.append(cur_generations)\n",
    "\n",
    "\n",
    "    with open('./data/' + save_string + '_fig4_embeddings.pkl', 'wb') as f:\n",
    "        pickle.dump(all_embeddings, f)\n",
    "    with open('./data/' + save_string + '_fig4_generations.pkl', 'wb') as f:\n",
    "        pickle.dump(all_generations, f)\n",
    "\n",
    "    model.cpu()\n",
    "    del model\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "_ = get_data(\"EleutherAI/gpt-j-6B\")\n",
    "_ = get_data(\"reciprocate/ppo_hh_gpt-j\")\n",
    "sphere_data = make_sphere_data([\"EleutherAI/gpt-j-6B\", \"reciprocate/ppo_hh_gpt-j\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "(10, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6438/3554192940.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print(np.array(sphere_data['EleutherAI/gpt-j-6B']).shape)\n"
     ]
    }
   ],
   "source": [
    "print(len(sphere_data['EleutherAI/gpt-j-6B'][2][3]))\n",
    "\n",
    "print(np.array(sphere_data['EleutherAI/gpt-j-6B']).shape)\n",
    "\n",
    "# sphere_data['reciprocate/ppo_hh_gpt-j']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_strings = [\"EleutherAI/gpt-j-6B\", \"reciprocate/ppo_hh_gpt-j\"]\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(model_strings[0])\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_strings[1])\n",
    "\n",
    "generations_by_model_string = {}\n",
    "for model_string in model_strings:\n",
    "    save_string = model_string.replace('/', '_')\n",
    "    with open('./data/' + save_string + '_fig4_generations.pkl', 'rb') as f:\n",
    "        generations_by_model_string[model_string] = pickle.load(f)\n",
    "\n",
    "model_strings_df = []\n",
    "labels_df = []\n",
    "coordinates_df = []\n",
    "prompt_or_continuation_df = []\n",
    "index_number_df = []\n",
    "\n",
    "for sample_idx in range(SUBSAMPLE_N):\n",
    "    all_traces = []\n",
    "    for i, coordinates1 in enumerate(sphere_data[model_strings[0]][sample_idx]):\n",
    "        labels1 = tokenizer1.tokenize(generations_by_model_string[model_strings[0]][sample_idx][i])\n",
    "\n",
    "        coordinates2 = sphere_data[model_strings[1]][sample_idx][i]\n",
    "        labels2 = tokenizer2.tokenize(generations_by_model_string[model_strings[1]][sample_idx][i])\n",
    "\n",
    "        k = 0\n",
    "        for j, t in enumerate(labels1):\n",
    "            if labels2[j] == t:\n",
    "                k +=1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if not i:\n",
    "            model_strings_df.append(model_strings[0])\n",
    "            labels_df.append(labels1[:k])\n",
    "            coordinates_df.append(coordinates1[:k])\n",
    "            prompt_or_continuation_df.append(\"prompt\")\n",
    "            index_number_df.append(i)\n",
    "\n",
    "            model_strings_df.append(model_strings[1])\n",
    "            labels_df.append(labels2[:k])\n",
    "            coordinates_df.append(coordinates2[:k])\n",
    "            prompt_or_continuation_df.append(\"prompt\")\n",
    "            index_number_df.append(i)\n",
    "\n",
    "        model_strings_df.append(model_strings[0])\n",
    "        labels_df.append(labels1[k:])\n",
    "        coordinates_df.append(coordinates1[k:])\n",
    "        prompt_or_continuation_df.append(\"continuation\")\n",
    "        index_number_df.append(i)\n",
    "\n",
    "        model_strings_df.append(model_strings[1])\n",
    "        labels_df.append(labels2[k:])\n",
    "        coordinates_df.append(coordinates2[k:])\n",
    "        prompt_or_continuation_df.append(\"continuation\")\n",
    "        index_number_df.append(i)\n",
    "\n",
    "df = pd.DataFrame(data={'model_strings': model_strings_df, 'labels': labels_df, 'coordinates': coordinates_df, 'prompt_or_continuation': prompt_or_continuation_df, 'index_number': index_number_df})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"tmp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
